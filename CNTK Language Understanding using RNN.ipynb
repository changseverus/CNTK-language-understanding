{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing locally cached: atis.test.ctf\n",
      "Reusing locally cached: atis.train.ctf\n",
      "Reusing locally cached: slots.wl\n",
      "Reusing locally cached: query.wl\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility function to download a file \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "\n",
    "locations = ['Tutorials/SLUHandsOn', 'Examples/LanguageUnderstanding/ATIS/BrainScript']\n",
    "\n",
    "data = {\n",
    "  'train': { 'file': 'atis.train.ctf', 'location': 0 },\n",
    "  'test': { 'file': 'atis.test.ctf', 'location': 0 },\n",
    "  'query': { 'file': 'query.wl', 'location': 1 },\n",
    "  'slots': { 'file': 'slots.wl', 'location': 1 }\n",
    "}\n",
    "\n",
    "for item in data.values():\n",
    "    location = locations[item['location']]\n",
    "    path = os.path.join('..', location, item['file'])\n",
    "    if os.path.exists(path):\n",
    "        print(\"Reusing locally cached:\", item['file'])\n",
    "        # Update path\n",
    "        item['file'] = path\n",
    "    elif os.path.exists(item['file']):\n",
    "        print(\"Reusing locally cached:\", item['file'])\n",
    "    else:\n",
    "        print(\"Starting download:\", item['file'])\n",
    "        url = \"https://github.com/Microsoft/CNTK/blob/v2.0/%s/%s?raw=true\"%(location, item['file'])\n",
    "        download(url, item['file'])\n",
    "        print(\"Download completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CNTK and other useful libraries\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the right target device when this notebook is being tested:\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        C.device.try_set_default_device(C.device.cpu())\n",
    "    else:\n",
    "        C.device.try_set_default_device(C.device.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in vocab, slot labels, and intent labels\n",
    "vocab_size = 943 ; num_labels = 129 ; num_intents = 26    \n",
    "\n",
    "# model dimensions\n",
    "input_dim  = vocab_size\n",
    "label_dim  = num_labels\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300\n",
    "\n",
    "# Create the containers for input feature (x) and the label (y)\n",
    "x = C.sequence.input_variable(vocab_size)\n",
    "y = C.sequence.input_variable(num_labels)\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1, 150)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# peek\n",
    "z = create_model()\n",
    "print(z.embed.E.shape)\n",
    "print(z.classify.b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 150)\n"
     ]
    }
   ],
   "source": [
    "# Pass an input and check the dimension\n",
    "z = create_model()\n",
    "print(z(x).embed.E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent_unused = C.io.StreamDef(field='S1', shape=num_intents, is_sparse=True),  \n",
    "         slot_labels   = C.io.StreamDef(field='S2', shape=num_labels,  is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['intent_unused', 'slot_labels', 'query'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek\n",
    "reader = create_reader(data['train']['file'], is_training=True)\n",
    "reader.streams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Composite(Combine): Input('Input2300', [#, *], [129]), Placeholder('labels', [???], [???]) -> Output('Block2270_Output_0', [#, *], [???]), Output('Block2290_Output_0', [#, *], [???])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_criterion_function(model):\n",
    "    labels = C.placeholder(name='labels')\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return C.combine ([ce, errs]) # (features, labels) -> (loss, metric)\n",
    "\n",
    "criterion = create_criterion_function(create_model())\n",
    "criterion.replace_placeholders({criterion.placeholders[0]: C.sequence.input_variable(num_labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criterion_function_preferred(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(reader, model_func, max_epochs=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 70\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    # (we don't run this many epochs, but if we did, these are good values)\n",
    "    lr_per_sample = [0.003]*4+[0.0015]*24+[0.0003]\n",
    "    lr_per_minibatch = [lr * minibatch_size for lr in lr_per_sample]\n",
    "    lr_schedule = C.learning_rate_schedule(lr_per_minibatch, C.UnitType.minibatch, epoch_size)\n",
    "    \n",
    "    # Momentum schedule\n",
    "    momentum_as_time_constant = C.momentum_as_time_constant_schedule(700)\n",
    "    \n",
    "    # We use a the Adam optimizer which is known to work well on this dataset\n",
    "    # Feel free to try other optimizers from \n",
    "    # https://www.cntk.ai/pythondocs/cntk.learner.html#module-cntk.learner\n",
    "    learner = C.adam(parameters=model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentum_as_time_constant,\n",
    "                     gradient_clipping_threshold_per_sample=15, \n",
    "                     gradient_clipping_with_truncation=True)\n",
    "\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    #progress_printer = ProgressPrinter(freq=100, first=10, tag='Training', num_epochs=max_epochs) \n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "                x: reader.streams.query,\n",
    "                y: reader.streams.slot_labels\n",
    "            })\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += data[y].num_samples                    # samples so far\n",
    "        trainer.summarize_training_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 721479 parameters in 6 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.692392 * 18010, metric = 14.14% * 18010 14.127s (1274.9 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.196757 * 18051, metric = 4.43% * 18051 9.989s (1807.1 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.127802 * 17941, metric = 2.88% * 17941 7.583s (2366.0 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.089395 * 18059, metric = 2.14% * 18059 7.789s (2318.5 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.057005 * 17957, metric = 1.33% * 17957 9.109s (1971.3 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.051911 * 18021, metric = 1.22% * 18021 8.221s (2192.1 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.047058 * 17980, metric = 1.17% * 17980 11.428s (1573.3 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.040754 * 18025, metric = 1.07% * 18025 11.094s (1624.8 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.024979 * 17956, metric = 0.69% * 17956 8.033s (2235.3 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.029398 * 18039, metric = 0.81% * 18039 7.921s (2277.4 samples/s);\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "def do_train():\n",
    "    global z\n",
    "    z = create_model()\n",
    "    reader = create_reader(data['train']['file'], is_training=True)\n",
    "    train(reader, z)\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "def evaluate(reader, model_func):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Create the loss and error functions\n",
    "    loss, label_error = create_criterion_function_preferred(model, y)\n",
    "\n",
    "    # process minibatches and perform evaluation\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Evaluation', num_epochs=0)\n",
    "\n",
    "    while True:\n",
    "        minibatch_size = 500\n",
    "        data = reader.next_minibatch(minibatch_size, input_map={  # fetch minibatch\n",
    "            x: reader.streams.query,\n",
    "            y: reader.streams.slot_labels\n",
    "        })\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "\n",
    "        evaluator = C.eval.Evaluator(loss, progress_printer)\n",
    "        evaluator.test_minibatch(data)\n",
    "     \n",
    "    evaluator.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-23]: metric = 0.34% * 10984;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-3.44756097e-02, -9.48742926e-02, -4.88143601e-02, -5.89377098e-02,\n",
       "       -1.66121200e-02, -4.74706963e-02, -4.71220650e-02, -9.88641307e-02,\n",
       "       -2.75263712e-02, -5.82716130e-02, -3.17683741e-02, -3.77656259e-02,\n",
       "       -5.17959744e-02, -5.13010696e-02, -8.52241889e-02, -7.09928051e-02,\n",
       "       -1.22144140e-01, -4.68127318e-02,  4.03327495e-02, -1.32786557e-01,\n",
       "       -7.63492659e-02, -5.49318455e-02,  5.47909038e-03, -7.66143017e-03,\n",
       "       -4.85182293e-02,  1.35481404e-02, -3.55821103e-02, -3.82119953e-03,\n",
       "       -1.73545275e-02, -3.66954207e-02, -3.91245857e-02, -3.24573815e-02,\n",
       "       -7.47283399e-02, -1.02308849e-02, -5.27024157e-02,  9.11082923e-02,\n",
       "        6.01251870e-02, -2.08287835e-02, -1.44987218e-02, -4.56239879e-02,\n",
       "       -1.41239911e-01, -6.53960556e-02,  1.86713673e-02, -4.64309864e-02,\n",
       "        1.97561365e-02, -8.04510191e-02,  3.41979191e-02,  2.72030365e-02,\n",
       "        2.97282673e-02, -4.36839908e-02, -6.07293919e-02, -8.45976621e-02,\n",
       "        2.59138849e-02, -6.07877225e-02,  4.82544228e-02,  1.19269500e-02,\n",
       "       -5.43233939e-02, -5.74248545e-02, -5.03415279e-02, -4.75362577e-02,\n",
       "       -1.24109052e-02, -6.74355775e-02,  1.81026626e-02, -2.64671147e-02,\n",
       "       -3.92723270e-02, -4.94098216e-02, -1.08448692e-01, -6.77800328e-02,\n",
       "       -5.91493733e-02, -1.15854375e-01, -8.16888884e-02, -5.41500933e-02,\n",
       "       -4.65299599e-02, -2.93784346e-02, -6.61164969e-02,  7.96463049e-04,\n",
       "       -5.50192688e-03,  2.57835649e-02, -5.99264912e-03, -4.49002646e-02,\n",
       "       -3.76229472e-02,  3.33864664e-05, -1.63201485e-02, -1.21627510e-01,\n",
       "       -3.10363099e-02, -8.13061297e-02, -7.82171935e-02, -1.07064201e-02,\n",
       "       -6.88804314e-02, -8.97430032e-02, -2.68881507e-02, -1.10482648e-02,\n",
       "       -9.09443200e-02, -1.11157157e-01, -4.34679538e-02, -1.03078388e-01,\n",
       "        6.60965964e-02, -3.63457054e-02, -7.38422796e-02, -9.15804803e-02,\n",
       "        2.02473681e-02, -8.69891271e-02, -7.64997080e-02, -8.81606936e-02,\n",
       "       -3.78384963e-02,  3.99602428e-02, -1.18180275e-01, -6.31501973e-02,\n",
       "       -7.66061321e-02, -9.24040377e-03, -5.86490408e-02, -4.98860702e-02,\n",
       "       -5.57233160e-03, -3.92621346e-02,  2.18868665e-02, -6.98483959e-02,\n",
       "        1.22968794e-03, -4.48514335e-02, -2.07176153e-02, -1.39159217e-01,\n",
       "       -1.19103104e-01, -4.93447334e-02, -6.27823174e-02, -7.37208501e-03,\n",
       "        2.59530428e-03, -2.87322327e-02, -4.19092029e-02, -7.76861683e-02,\n",
       "        1.08806334e-01], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_test():\n",
    "    reader = create_reader(data['test']['file'], is_training=False)\n",
    "    evaluate(reader, z)\n",
    "do_test()\n",
    "z.classify.b.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178, 429, 444, 619, 937, 851, 752, 179]\n",
      "(8, 129)\n",
      "[128 128 128  48 110 128  78 128]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('BOS', 'O'),\n",
       " ('flights', 'O'),\n",
       " ('from', 'O'),\n",
       " ('new', 'B-fromloc.city_name'),\n",
       " ('york', 'I-fromloc.city_name'),\n",
       " ('to', 'O'),\n",
       " ('seattle', 'B-toloc.city_name'),\n",
       " ('EOS', 'O')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "slots_wl = [line.rstrip('\\n') for line in open(data['slots']['file'])]\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "slots_dict = {slots_wl[i]:i for i in range(len(slots_wl))}\n",
    "\n",
    "# let's run a sequence through\n",
    "seq = 'BOS flights from new york to seattle EOS'\n",
    "w = [query_dict[w] for w in seq.split()] # convert to word indices\n",
    "print(w)\n",
    "onehot = np.zeros([len(w),len(query_dict)], np.float32)\n",
    "for t in range(len(w)):\n",
    "    onehot[t,w[t]] = 1\n",
    "\n",
    "#x = C.sequence.input_variable(vocab_size)\n",
    "pred = z(x).eval({x:[onehot]})[0]\n",
    "print(pred.shape)\n",
    "best = np.argmax(pred,axis=1)\n",
    "print(best)\n",
    "list(zip(seq.split(),[slots_wl[s] for s in best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch normalization\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "# Enable these when done:\n",
    "z = create_model()\n",
    "#do_train()\n",
    "#do_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lookahead\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels)\n",
    "        ])\n",
    "    \n",
    "# Enable these when done:\n",
    "z = create_model()\n",
    "#do_train()\n",
    "#do_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bidirectional recurrence\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):  \n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "# Enable these when done:\n",
    "#do_train()\n",
    "#do_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Adding Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 721479 parameters in 6 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.774070 * 18010, metric = 15.11% * 18010 6.858s (2626.1 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.223397 * 18051, metric = 5.12% * 18051 7.450s (2423.0 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.153779 * 17941, metric = 3.51% * 17941 8.274s (2168.4 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.107101 * 18059, metric = 2.62% * 18059 8.326s (2169.0 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.068279 * 17957, metric = 1.60% * 17957 10.771s (1667.2 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.061056 * 18021, metric = 1.43% * 18021 11.655s (1546.2 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.052172 * 17980, metric = 1.22% * 17980 7.706s (2333.2 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.048326 * 18025, metric = 1.23% * 18025 8.932s (2018.0 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.032795 * 17956, metric = 0.90% * 17956 7.779s (2308.3 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.033729 * 18039, metric = 0.88% * 18039 9.978s (1807.9 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-23]: metric = 0.32% * 10984;\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim),\n",
    "            #C.layers.BatchNormalization(),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            #C.layers.BatchNormalization(),\n",
    "            C.layers.Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Add a Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 901479 parameters in 6 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.623953 * 18010, metric = 12.71% * 18010 12.903s (1395.8 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.166708 * 18051, metric = 3.72% * 18051 10.257s (1759.9 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.108443 * 17941, metric = 2.37% * 17941 11.541s (1554.5 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.068857 * 18059, metric = 1.57% * 18059 7.845s (2302.0 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.040734 * 17957, metric = 0.99% * 17957 11.045s (1625.8 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.036186 * 18021, metric = 0.81% * 18021 13.684s (1316.9 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.039799 * 17980, metric = 0.99% * 17980 10.951s (1641.9 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.028615 * 18025, metric = 0.79% * 18025 11.143s (1617.6 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.016580 * 17956, metric = 0.42% * 17956 10.658s (1684.7 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.015819 * 18039, metric = 0.43% * 18039 12.371s (1458.2 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-23]: metric = 0.30% * 10984;\n"
     ]
    }
   ],
   "source": [
    "def OneWordLookahead():\n",
    "    x = C.placeholder()\n",
    "    apply_x = C.splice(x, C.sequence.future_value(x))\n",
    "    return apply_x\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim),\n",
    "            OneWordLookahead(),\n",
    "            C.layers.Recurrence(C.layers.LSTM(hidden_dim), go_backwards=False),\n",
    "            C.layers.Dense(num_labels)        \n",
    "        ])\n",
    "\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: Bidirectional Recurrent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 541479 parameters in 9 parameter tensors.\n",
      "Learning rate per minibatch: 0.21\n",
      "Finished Epoch[1 of 10]: [Training] loss = 0.723112 * 18010, metric = 13.59% * 18010 9.051s (1989.8 samples/s);\n",
      "Finished Epoch[2 of 10]: [Training] loss = 0.173359 * 18051, metric = 4.02% * 18051 13.087s (1379.3 samples/s);\n",
      "Finished Epoch[3 of 10]: [Training] loss = 0.104300 * 17941, metric = 2.20% * 17941 9.784s (1833.7 samples/s);\n",
      "Finished Epoch[4 of 10]: [Training] loss = 0.066566 * 18059, metric = 1.59% * 18059 9.216s (1959.5 samples/s);\n",
      "Learning rate per minibatch: 0.105\n",
      "Finished Epoch[5 of 10]: [Training] loss = 0.038120 * 17957, metric = 0.88% * 17957 12.410s (1447.0 samples/s);\n",
      "Finished Epoch[6 of 10]: [Training] loss = 0.038607 * 18021, metric = 0.89% * 18021 9.877s (1824.5 samples/s);\n",
      "Finished Epoch[7 of 10]: [Training] loss = 0.035566 * 17980, metric = 0.90% * 17980 11.005s (1633.8 samples/s);\n",
      "Finished Epoch[8 of 10]: [Training] loss = 0.027885 * 18025, metric = 0.70% * 18025 8.636s (2087.2 samples/s);\n",
      "Finished Epoch[9 of 10]: [Training] loss = 0.015278 * 17956, metric = 0.40% * 17956 8.338s (2153.5 samples/s);\n",
      "Finished Epoch[10 of 10]: [Training] loss = 0.015865 * 18039, metric = 0.48% * 18039 8.981s (2008.6 samples/s);\n",
      "Finished Evaluation [1]: Minibatch[1-23]: metric = 0.42% * 10984;\n"
     ]
    }
   ],
   "source": [
    "def BiRecurrence(fwd, bwd):\n",
    "    F = C.layers.Recurrence(fwd)\n",
    "    G = C.layers.Recurrence(bwd, go_backwards=True)\n",
    "    x = C.placeholder()\n",
    "    apply_x = C.splice(F(x), G(x))\n",
    "    return apply_x \n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim),\n",
    "            BiRecurrence(C.layers.LSTM(hidden_dim//2), \n",
    "                                  C.layers.LSTM(hidden_dim//2)),\n",
    "            C.layers.Dense(num_labels)\n",
    "        ])\n",
    "\n",
    "do_train()\n",
    "do_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
